{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a FAISS-Based Vector Store: A Journey Through Data Processing and Visualization\n",
    "\n",
    "In this notebook, you'll learn how to transform raw PDF documents into a searchable vector store using FAISS. We'll go on a journey where we:\n",
    "\n",
    "1. **Read and extract text from PDF files.**\n",
    "2. **Split the text into manageable chunks.**\n",
    "3. **Display tokenization outputs from different tokenizers.**\n",
    "4. **Generate embeddings from the text using a SentenceTransformer.**\n",
    "5. **Store the embeddings in a FAISS index.**\n",
    "6. **Project the embeddings into 2D space using UMAP for visualization.**\n",
    "7. **Visualize the entire process on a scatter plot.**\n",
    "8. **Incect your data into a prompt for a large language model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#können nicht einfach importiert werden. Also:\n",
    "#pip install tqdm\n",
    "#pip install PyPDF2\n",
    "#pip install langchain\n",
    "#pip install sentence-transformers\n",
    "#pip install faiss-cpu\n",
    "#pip install matplotlib\n",
    "#pip install umap-learn\n",
    "#pip install numpy\n",
    "\n",
    "#langchain_community.embeddings geht teils nicht. Dann: pip install --upgrade langchain langchain-community\n",
    "\n",
    "import tqdm\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # For generating embeddings for text chunks\n",
    "import faiss\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading Data from PDFs\n",
    "\n",
    "First, we load PDF files from a directory, extract their text content, and combine it into one large text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Building Competence. Crossing Borders.Retrieval Au'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#* heisst einfach, dass ein PDF von data geladen wird\n",
    "\n",
    "### load the pdf from the path\n",
    "glob_path = \"data/*.pdf\"\n",
    "text = \"\"\n",
    "for pdf_path in tqdm.tqdm(glob.glob(glob_path)):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PdfReader(file)\n",
    "         # Extract text from all pages in the PDF\n",
    "        text += \" \".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "\n",
    "text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "## 2. Splitting the Text into Chunks\n",
    "\n",
    "Large texts can be difficult to work with. We use a text splitter to break the full text into smaller, overlapping chunks. This helps preserve context when we later embed the text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Building Competence. Crossing Borders.Retrieval Augmented Generation\\nTokenization and Embedding\\nJasmin Heierli \\njasm in.heierli@zhaw.ch / 18.03.2024 RAG Architecture\\n2 NLP –Week 4 18.03.2024User Input Data Retrieval\\n(Embeddings, \\nVektor database)Prompt Injection Large Language Model AnswerData Storage/Source New Problem\\n3−Given our user query: What are the first words spoken after landing on the moon?\\n−Given a knowledge base (knowledge graph, database, collection of texts…)\\n−How canweidentify relevant knowledge base entries ?\\nqueryanswerwrong answer\\nEuclidian Distance\\nCosine of\\nangle \\nbetween\\nvectors\\n18.03.2024 NLP –Week 4 Semantics\\n4Time flies like an arrow; fruit flies like a banana\\nfly: moving through theair.\\nfly: a small insect withsmall wings\\nhttps://web.stanford.edu/~jurafsky/slp3/6.pdf\\nhttps://web.stanford.edu/~jurafsky/slp3/G.pdfSemantics in a nutshell: A fundamental aspect of semantics is the recognition that there is not \\nalways a direct connection between words and their meanings. This concept challenges the \\nintuitive belief that words are inherently tied to what they represent.\\n18.03.2024 NLP –Week 4 Word Representation\\n5 Empty field for document title 12.1.2022−Machine Learning Models cannot read raw\\ntext\\n−LMs seem tobeable to\\nWhy then do westore embeddings and not raw\\ntextin a vector store ?\\n→Indexing and search happens based on text\\nsimilarity\\n→Neither canbesufficiently computed based\\non characters\\nhttps://www.gutenberg.org/ebooks/2701 Tokenization\\n6−Originally thetask ofsegmenting a textintowords\\n−Tokenization : The process ofsegmenting documents intoparagraphs , sentences , and \\nwords (tokens ) per sentence .\\nText\\n„A boiled egg in themorning ishard tobeat.“\\nTokens\\n„A“, „ boiled “, „egg“, „in“, „ the“, „morning “, „is“, „hard“ „to“, \\n„beat“, „.“Space -based tokenization :\\nsimplest heuristic tosplittext\\nforsome languages\\nSource: Carstensen et al., Computerlinguistik und \\nSprachtechnologie: Eine Einführung , Spektrum \\nAkademischer Verlag Heidelberg',\n",
       " 'simplest heuristic tosplittext\\nforsome languages\\nSource: Carstensen et al., Computerlinguistik und \\nSprachtechnologie: Eine Einführung , Spektrum \\nAkademischer Verlag Heidelberg\\n18.03.2024 NLP –Week 4 Problem: New Words\\n7Problem: Youbuilt your perfect linguistic tokenizer on a training corpus and your actual data\\nthatyouwork with, contains a word ithasnot seen before .\\nExample\\nTraining corpus contains : low, high, cat, \\nstairs\\nTest corpus contains : dog\\nDo youstill think thatlinguistic word tokenizations isa good solution tobuild realworld\\napplications ?\\n18.03.2024 NLP –Week 4 Solution: Subword tokenisation\\n8Modern tokenizers induce ideal tokens thatareoften smaller than words , called subwords .\\nThey may correspond tomorphemes1, but they don‘t have to.\\nByte -Pair Encoding: Bottom -upapproach , weuseourdata totelluswhat theideal token\\nshould be\\n1 smallest unitofmeaning in a wordhttps://web.stanford.edu/~jurafsky/slp3/2.pdf\\n18.03.2024 NLP –Week 4 Byte -Pair Encoding\\n9−Token -learner begins with a set of tokens that is just a \\nset of all individual characters\\n−Then it starts to merge the two most frequent co -\\noccurring characters together.\\n−It continues to count and merge together sequences, \\ncreating more and longer sequences\\n−The alogorithm usually respects word boundaries if \\nexisting\\n−Often ends up learning entire words from the training \\ncorpus.\\n−A token segmenter then greedily splits each test \\nsentence into characters and then starts to apply the \\nmerging rules.\\nhttps://web.stanford.edu/~jurafsky/slp3/2.pdf\\n18.03.2024 NLP –Week 4 From Tokens toNumbers\\n10A traditional way to encode texts as numbers: one-hot-encoding\\nExample: for web search you want to match “London cab” in documents containing “London \\ntaxi”\\nResult: \\ncab = [ 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 ]\\ntaxi = [ 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ]Problem\\n−The vectors are orthogonal\\n−There is no natural notion of similarity for \\none-hot-encoded vectors\\n−They become immensely sparse with a \\nlarge vocabulary',\n",
       " \"−The vectors are orthogonal\\n−There is no natural notion of similarity for \\none-hot-encoded vectors\\n−They become immensely sparse with a \\nlarge vocabulary\\n−The integer -encoding does not capture \\nany relationship between words\\n18.03.2024 NLP –Week 4 From Tokens toNumbers with Context\\n11\\nA word's meaning can be inferred by the \\ncontext it appears in. Synonyms (words \\nwith similar meaning s) often appear in \\nsimilar contexts.\\nWe can use all the available contexts of a \\ngiven word to build up\\nArepresentation of it.\\nhttps:// twitter.com /data_monsters\\nhttps:// web.stanford.edu /~jurafsky /slp3/6.pdf\\n18.03.2024 NLP –Week 4 Word Embeddings\\n12An embedding is a short dense vector of a fixed size consisting \\nof floating point numbers. Similar words have a similar \\nencoding.  \\nStatic embeddings : the same token has always the same \\nembedding\\nContextual embeddings: different representations for a token \\ndepending on its context.\\nThe floating point values of the embedding vector are trainable \\nparameters and the weights can be learned by a neural \\nnetwork. You just need to specify the amount of dimensions \\n(typically between 100 and 2000)\\nhttps:// web.stanford.edu /~jurafsky /slp3/6.pdf\\n18.03.2024NLP –Week 4 Word Embeddings\\n13 NLP –Week 4 18.03.2024-Beginning :Each word isassigned avector with\\nrandom numbers\\n-During training vectors aremodified insuch a\\nwaythatthey better represent each word in \\nrelation toall other words .\\n−Example :Play andgames start outwithrandom\\nembeddings andconverge during training\\n-Unsupervised !\\nhttps://medium.com/womenintechnology/transformers -for-dummies -\\nword -embeddings -1c9ade5ff500 Word Embeddings\\n14 Empty field for document title18.03.2024−Dense vector for each word, calculated so that \\nwords that appear in similar contexts have \\nsimilar vectors.\\n−Word embeddings are a distributed \\nrepresentation\\n−Since word embeddings are vectors, you can \\ncalculate: distance, sum, difference, product \\n→Cosine similarity is most commonly used to\",\n",
       " '−Word embeddings are a distributed \\nrepresentation\\n−Since word embeddings are vectors, you can \\ncalculate: distance, sum, difference, product \\n→Cosine similarity is most commonly used to \\ncalculate the similarity between two vectors\\nhttps:// web.stanford.edu /~jurafsky /slp3 Word Embeddings\\n15 NLP –Week 4 18.03.2024−Weneed toaddposition in order tocapture\\nnuances in meaning\\n−Naive way: Add sentence position tovector\\n→No:cansignificantly distort emebeddings , \\nespecially , forwords at theendoflong\\nsentences /chunks\\n−Fractions ?Aword attheposition 2 hasa value of\\n2/30 in a 30 word sentence and 2/50 in a 50 \\nword sentence …\\nhttps://medium.com/womenintechnology/transformers -for-dummies -positional -embeddings -bec8474ed6f5 16 NLP –Week 4 18.03.2024-Use ofSinus and Cosinus functions !\\n-They are smooth and continuous, meaning small \\nposition changes lead to small changes in the \\nvalues.\\n-The values are independent of sentence length \\nbecause the frequency of the waves can be \\nadjusted.\\n-Each wave corresponds to a different index of \\nthe embedding vector\\n-The x-axis represents the word position in the \\nsentence, and each wave corresponds to a \\ndifferent index iof the positional embedding .\\nhttps ://medium.com/womenintechnology/transformers -for-dummies -positional -embeddings -bec8474ed6f5 Cosine Similarity\\n17Let‘s close thecircle and goback toouroriginal Problem:\\nWecannow calculate thecosine similarity between twovectors\\nQuery [[50, 10]]Answer [[48, 12]]Wrong Answer [[12, 48]]\\n18.03.2024 NLP –Week 4 18https://projector.tensorflow.org/Visualisation ofa Word Embedding Space\\n18.03.2024 NLP –Week 4 Thank you.\\n12.1.2022 Empty field for document title 1919\\nThank you.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a splitter: 2000 characters per chunk with an overlap of 200 characters\n",
    "\n",
    "# Initialize a text splitter with specified chunk size and overlap\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "# Split the extracted text into manageable chunks\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "# Display the resulting chunks\n",
    "chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 4\n",
      "Preview of the first chunk: Building Competence. Crossing Borders.Retrieval Augmented Generation\n",
      "Tokenization and Embedding\n",
      "Jasmin Heierli \n",
      "jasm in.heierli@zhaw.ch / 18.03.2024 RAG Architecture\n",
      "2 NLP –Week 4 18.03.2024User Input\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(\"Preview of the first chunk:\", chunks[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenizing the Text with Different Tokenizers\n",
    "\n",
    "Before embedding, it's insightful to see how different tokenizers break up our text. Here, we use the tokenizer from the SentenceTransformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ein Tokenizer zerlegt Text in kleinere Einheiten, sogenannte Tokens. Diese Tokens sind typischerweise: Wörter, Wortbestandteile oder einzelne Zeichen\n",
    "#\"Das ist ein Test.\" Tokens könnten sein: [\"Das\", \"ist\", \"ein\", \"Test\", \".\"]\n",
    "\n",
    "#SentenceTransformersTokenTextSplitter --> ist ein spezialisierter Text-Splitter, der den Tokenizer des angegebenen Modells verwendet.\n",
    "#model_name=\"paraphrase-multilingual-MiniLM-L12-v2\" --> bedeutet: Es wird der Tokenizer von diesem Multilingualen MiniLM-Modell genutzt.\n",
    "#tokens_per_chunk=128 --> Der Text wird in Chunks von maximal 128 Tokens aufgeteilt.\n",
    "#chunk_overlap=0 --> Es gibt keine Überlappung zwischen den Chunks.\n",
    "\n",
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=128, model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total chunks: 19\n",
      "Building Competence. Crossing Borders.Retrieval Augmented Generation Tokenization and Embedding Jasmin Heierli jasm in.heierli@zhaw.ch / 18.03.2024 RAG Architecture 2 NLP –Week 4 18.03.2024User Input Data Retrieval (Embeddings, Vektor database)Prompt Injection Large Language Model AnswerData Storage/Source New Problem 3−Given our user query: What are the first words spoken after landing on the moon? −Given a knowledge base (knowledge graph,\n"
     ]
    }
   ],
   "source": [
    "#Du hast eine Liste von Textabschnitten (chunks) – und willst diese weiter in kleinere Textteile splitten, basierend auf der Token-Anzahl.\n",
    "\n",
    "token_split_texts = [] #erstellt leere Liste\n",
    "for text in chunks:     #Für jeden Textabschnitt (text) in deiner chunks-Liste. wende token_splitter.split_text(text) an – das verwendet deinen vorher definierten Token-basierten Splitter (z. B. in 128-Tokens-große Stücke), und füge die gesplitteten Textstücke zu token_split_texts hinzu (per +=).\n",
    "    token_split_texts += token_splitter.split_text(text)\n",
    "print(f\"\\nTotal chunks: {len(token_split_texts)}\")\n",
    "print(token_split_texts[0])   #Zeigt dir das erste Textstück, das per Tokenizer gesplittet wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: ['<s>', '▁Building', '▁Comp', 'et', 'ence', '.', '▁Cross', 'ing', '▁Bord', 'ers', '.', 'Re', 'trie', 'val', '▁Aug', 'mente', 'd', '▁Generation', '▁To', 'ken', 'ization', '▁and', '▁Em', 'be', 'dding', '▁Jasmin', '▁Hei', 'er', 'li', '▁ja', 's', 'm', '▁in', '.', 'hei', 'er', 'li', '@', 'zh', 'aw', '.', 'ch', '▁/', '▁18.', '03.20', '24', '▁RA', 'G', '▁Architect', 'ure', '▁2', '▁N', 'LP', '▁–', 'We', 'ek', '▁4', '▁18.', '03.20', '24', 'U', 'ser', '▁In', 'put', '▁Data', '▁Re', 'trie', 'val', '▁(', 'Em', 'be', 'dding', 's', ',', '▁V', 'ektor', '▁database', ')', 'Pro', 'mp', 't', '▁In', 'je', 'ction', '▁Large', '▁Language', '▁Model', '▁Answer', 'Data', '▁Storage', '/', 'Source', '▁New', '▁Problem', '▁3', '−', 'Gi', 'ven', '▁our', '▁user', '▁que', 'ry', ':', '▁What', '▁are', '▁the', '▁first', '▁words', '▁spoke', 'n', '▁after', '▁landing', '▁on', '▁the', '▁', 'moon', '?', '▁−', 'Gi', 'ven', '▁a', '▁knowledge', '▁base', '▁(', 'know', 'ledge', '▁', '</s>']\n",
      "Chunk 1: ['<s>', '▁database', ',', '▁collection', '▁of', '▁text', 's', '...)', '▁−', 'How', '▁can', 'we', 'ident', 'ify', '▁relevant', '▁knowledge', '▁base', '▁entri', 'es', '▁?', '▁quer', 'yan', 's', 'wer', 'w', 'rong', '▁answer', '▁Eu', 'c', 'li', 'dian', '▁Distan', 'ce', '▁Co', 'sine', '▁of', '▁angle', '▁between', '▁vec', 'tors', '▁18.', '03.20', '24', '▁N', 'LP', '▁–', 'We', 'ek', '▁4', '▁Sem', 'antic', 's', '▁4', 'Time', '▁fli', 'es', '▁like', '▁an', '▁', 'arrow', ';', '▁fruit', '▁fli', 'es', '▁like', '▁a', '▁banana', '▁fly', ':', '▁moving', '▁through', '▁the', 'air', '.', '▁fly', ':', '▁a', '▁small', '▁insect', '▁with', 's', 'mall', '▁w', 'ings', '▁https', '://', 'web', '.', 'stan', 'ford', '.', 'edu', '/', '~', 'ju', 'raf', 'sky', '/', 's', 'lp', '3', '/6', '.', 'pdf', '▁https', '://', 'web', '.', 'stan', 'ford', '.', 'edu', '/', '~', 'ju', 'raf', 'sky', '/', 's', 'lp', '3/', 'G', '.', 'pdf', 'Se', 'man', 'tics', '</s>']\n",
      "Chunk 2: ['<s>', '▁nut', 'shell', ':', '▁A', '▁fundamental', '▁aspect', '▁of', '▁sem', 'antic', 's', '▁is', '▁the', '▁recognition', '▁that', '▁there', '▁is', '▁not', '▁always', '▁a', '▁direct', '▁connection', '▁between', '▁words', '▁and', '▁their', '▁meaning', 's', '.', '▁This', '▁concept', '▁challenges', '▁the', '▁intuitiv', 'e', '▁belief', '▁that', '▁words', '▁are', '▁inherent', 'ly', '▁tied', '▁to', '▁what', '▁they', '▁represent', '.', '▁18.', '03.20', '24', '▁N', 'LP', '▁–', 'We', 'ek', '▁4', '▁Word', '▁Representa', 'tion', '▁5', '▁Emp', 'ty', '▁field', '▁for', '▁document', '▁title', '▁12', '.1.', '2022', '−', 'Ma', 'chine', '▁Learning', '▁Model', 's', '▁cannot', '▁read', '▁raw', '▁text', '▁−', 'LM', 's', '▁seem', '▁to', 'be', 'able', '▁to', '▁Why', '▁then', '▁do', '▁we', 'store', '▁', 'embe', 'dding', 's', '▁and', '▁not', '▁raw', '▁text', 'in', '▁a', '▁ve', 'ctor', '▁store', '▁?', '▁→', 'In', 'dex', 'ing', '▁and', '▁search', '▁happens', '▁based', '▁on', '▁text', '▁similar', 'ity', '▁→', 'Ne', 'i', 'ther', '▁can', 'be', 'sufficient', 'ly', '▁com', '</s>']\n",
      "Chunk 3: ['<s>', '▁based', '▁on', '▁characters', '▁https', '://', 'www', '.', 'gu', 'tenberg', '.', 'org', '/', 'e', 'books', '/2', '701', '▁To', 'ken', 'ization', '▁6', '−', 'Original', 'ly', '▁the', 'tas', 'k', '▁of', 'seg', 'ment', 'ing', '▁a', '▁text', 'into', 'word', 's', '▁−', 'To', 'ken', 'ization', '▁:', '▁The', '▁process', '▁of', 'seg', 'ment', 'ing', '▁documents', '▁into', 'para', 'graph', 's', '▁', ',', '▁sentence', 's', '▁', ',', '▁and', '▁words', '▁(', 'tok', 'ens', '▁)', '▁per', '▁sentence', '▁', '.', '▁Text', '▁„', 'A', '▁bo', 'i', 'led', '▁egg', '▁in', '▁the', 'mor', 'ning', '▁is', 'hard', '▁to', 'beat', '.', '“', '▁To', 'kens', '▁„', 'A', '“,', '▁„', '▁bo', 'i', 'led', '▁“', ',', '▁„', 'e', 'gg', '“,', '▁„', 'in', '“,', '▁„', '▁the', '“,', '▁„', 'mor', 'ning', '▁“', ',', '▁„', 'is', '“,', '▁„', 'hard', '“', '▁„', 'to', '“,', '▁„', 'beat', '“,', '▁„', '.', '“', 'Space', '</s>']\n",
      "Chunk 4: ['<s>', '▁to', 'ken', 'ization', '▁:', '▁simples', 't', '▁he', 'ur', 'istic', '▁to', 'sp', 'lit', 'text', '▁for', 'some', '▁language', 's', '▁Source', ':', '▁Car', 'sten', 'sen', '▁et', '▁al', '.', ',', '▁Computer', 'lingu', 'istik', '▁und', '▁Sprach', 'technologie', ':', '▁Eine', '▁Einführung', '▁', ',', '▁Spektrum', '▁Akademi', 'scher', '▁Verlag', '▁Heidelberg', '</s>']\n",
      "Chunk 5: ['<s>', '▁simples', 't', '▁he', 'ur', 'istic', '▁to', 'sp', 'lit', 'text', '▁for', 'some', '▁language', 's', '▁Source', ':', '▁Car', 'sten', 'sen', '▁et', '▁al', '.', ',', '▁Computer', 'lingu', 'istik', '▁und', '▁Sprach', 'technologie', ':', '▁Eine', '▁Einführung', '▁', ',', '▁Spektrum', '▁Akademi', 'scher', '▁Verlag', '▁Heidelberg', '▁18.', '03.20', '24', '▁N', 'LP', '▁–', 'We', 'ek', '▁4', '▁Problem', ':', '▁New', '▁Word', 's', '▁7', 'Problem', ':', '▁You', 'bu', 'ilt', '▁your', '▁perfect', '▁linguis', 'tic', '▁to', 'ken', 'izer', '▁on', '▁a', '▁training', '▁corpus', '▁and', '▁your', '▁actual', '▁data', '▁that', 'you', 'work', '▁with', ',', '▁contain', 's', '▁a', '▁word', '▁it', 'has', 'not', '▁seen', '▁before', '▁', '.', '▁Exam', 'ple', '▁Training', '▁corpus', '▁contain', 's', '▁:', '▁low', ',', '▁high', ',', '▁cat', ',', '▁stair', 's', '▁Test', '▁corpus', '▁contain', 's', '▁:', '▁dog', '▁Do', '▁you', 'still', '▁think', '▁that', 'lingu', 'istic', '▁word', '▁to', 'ken', 'ization', 's', '▁isa', '▁good', '▁solution', '▁to', '</s>']\n",
      "Chunk 6: ['<s>', '▁world', '▁applications', '▁?', '▁18.', '03.20', '24', '▁N', 'LP', '▁–', 'We', 'ek', '▁4', '▁Solution', ':', '▁Sub', 'word', '▁to', 'ken', 'isation', '▁8', 'Mo', 'dern', '▁to', 'ken', 'izer', 's', '▁induc', 'e', '▁ideal', '▁to', 'kens', '▁that', 'are', 'o', 'ften', '▁smaller', '▁than', '▁words', '▁', ',', '▁called', '▁sub', 'word', 's', '▁', '.', '▁They', '▁may', '▁correspond', '▁to', 'morph', 'em', 'es', '1', ',', '▁but', '▁they', '▁don', '‘', 't', '▁have', '▁to', '.', '▁By', 'te', '▁-', 'Pa', 'ir', '▁En', 'co', 'ding', ':', '▁Bot', 'tom', '▁-', 'u', 'pap', 'pro', 'ach', '▁', ',', '▁we', 'use', 'our', 'data', '▁tot', 'ellus', 'what', '▁the', 'ideal', '▁to', 'ken', '▁should', '▁be', '▁1', '▁small', 'est', '▁unit', 'of', 'mea', 'ning', '▁in', '▁a', '▁word', 'https', '://', 'web', '.', 'stan', 'ford', '.', 'edu', '/', '~', 'ju', 'raf', 'sky', '/', 's', 'lp', '3/', '2.', 'pdf', '▁18.', '03.20', '24', '</s>']\n",
      "Chunk 7: ['<s>', '▁–', 'We', 'ek', '▁4', '▁By', 'te', '▁-', 'Pa', 'ir', '▁En', 'co', 'ding', '▁9', '−', 'To', 'ken', '▁-', 'le', 'ar', 'ner', '▁begin', 's', '▁with', '▁a', '▁set', '▁of', '▁to', 'kens', '▁that', '▁is', '▁just', '▁a', '▁set', '▁of', '▁all', '▁individual', '▁characters', '▁−', 'The', 'n', '▁it', '▁start', 's', '▁to', '▁merge', '▁the', '▁two', '▁most', '▁frequent', '▁co', '▁-', '▁occur', 'ring', '▁characters', '▁together', '.', '▁−', 'It', '▁continues', '▁to', '▁count', '▁and', '▁merge', '▁together', '▁se', 'quen', 'ces', ',', '▁creating', '▁more', '▁and', '▁longer', '▁se', 'quen', 'ces', '▁−', 'The', '▁a', 'logo', 'rit', 'hm', '▁usually', '▁respect', 's', '▁word', '▁bo', 'unda', 'ries', '▁if', '▁existing', '▁−', 'O', 'ften', '▁end', 's', '▁up', '▁learning', '▁entire', '▁words', '▁from', '▁the', '▁training', '▁corpus', '.', '▁−', 'A', '▁to', 'ken', '▁segment', 'er', '▁then', '▁gre', 'ed', 'ily', '▁split', 's', '▁each', '▁test', '▁sentence', '▁into', '▁characters', '▁and', '▁then', '▁start', 's', '▁to', '</s>']\n",
      "Chunk 8: ['<s>', '▁mer', 'ging', '▁rules', '.', '▁https', '://', 'web', '.', 'stan', 'ford', '.', 'edu', '/', '~', 'ju', 'raf', 'sky', '/', 's', 'lp', '3/', '2.', 'pdf', '▁18.', '03.20', '24', '▁N', 'LP', '▁–', 'We', 'ek', '▁4', '▁From', '▁To', 'kens', '▁to', 'Nu', 'mber', 's', '▁10', 'A', '▁traditional', '▁way', '▁to', '▁en', 'code', '▁text', 's', '▁as', '▁numbers', ':', '▁one', '-', 'hot', '-', 'enco', 'ding', '▁Exam', 'ple', ':', '▁for', '▁web', '▁search', '▁you', '▁want', '▁to', '▁match', '▁“', 'London', '▁cab', '”', '▁in', '▁documents', '▁contain', 'ing', '▁“', 'London', '▁taxi', '”', '▁Result', ':', '▁cab', '▁=', '▁[', '▁0', '▁0', '▁0', '▁0', '▁0', '▁0', '▁0', '▁0', '▁0', '▁0', '▁0', '▁1', '▁0', '▁0', '▁0', '▁0', '▁]', '▁taxi', '▁=', '▁[', '▁0', '▁0', '▁0', '▁0', '▁0', '▁0', '▁0', '▁0', '▁1', '▁0', '▁0', '▁0', '▁0', '▁0', '▁0', '▁0', '▁]', 'Problem', '▁−', 'The', '▁vec', 'tors', '</s>']\n",
      "Chunk 9: ['<s>', '▁tho', 'gon', 'al', '▁−', 'There', '▁is', '▁no', '▁natural', '▁no', 'tion', '▁of', '▁similar', 'ity', '▁for', '▁one', '-', 'hot', '-', 'enco', 'ded', '▁vec', 'tors', '▁−', 'The', 'y', '▁become', '▁immens', 'ely', '▁spar', 'se', '▁with', '▁a', '▁large', '▁vocabula', 'ry', '</s>']\n"
     ]
    }
   ],
   "source": [
    "#zeigt wie ein Textstück in Tokens umgewandelt wird.\n",
    "\n",
    "#Du lädst das gewählte SentenceTransformer-Modell, inklusive Tokenizer.\n",
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "tokenized_chunks = []\n",
    "for i, text in enumerate(token_split_texts[:10]):      #Du gehst die ersten 10 Chunks deiner zuvor gesplitteten Texte durch.\n",
    "    # Tokenize each chunk\n",
    "    encoded_input = model.tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = model.tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0].tolist())\n",
    "    tokenized_chunks.append(tokens)\n",
    "    print(f\"Chunk {i}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: ['[CLS]', 'Bu', '##ilding', 'Comp', '##eten', '##ce', '.', 'Cross', '##ing', 'Borde', '##rs', '.', 'Ret', '##rie', '##val', 'Aug', '##mente', '##d', 'Generation', 'Tok', '##eni', '##za', '##tion', 'and', 'Em', '##bed', '##ding', 'Jas', '##min', 'Hei', '##erl', '##i', 'ja', '##sm', 'in', '.', 'hei', '##erl', '##i', '@', 'z', '##haw', '.', 'ch', '/', '18', '.', '03', '.', '202', '##4', 'R', '##AG', 'Archite', '##cture', '2', 'NL', '##P', '–', 'We', '##ek', '4', '18', '.', '03', '.', '202', '##4', '##U', '##ser', 'In', '##put', 'Data', 'Ret', '##rie', '##val', '(', 'Em', '##bed', '##dings', ',', 'Ve', '##ktor', 'dat', '##aba', '##se', ')', 'Prom', '##pt', 'In', '##ject', '##ion', 'Lar', '##ge', 'Lang', '##uage', 'Model', 'Ans', '##wer', '##Dat', '##a', 'Stor', '##age', '/', 'Source', 'New', 'Problem', '3', '##−', '##Gi', '##ven', 'our', 'use', '##r', 'quer', '##y', ':', 'What', 'are', 'the', 'first', 'wor', '##ds', 'sp', '##ok', '##en', 'after', '[SEP]']\n",
      "Chunk 1: ['[CLS]', 'dat', '##aba', '##se', ',', 'col', '##le', '##ction', 'of', 'te', '##xt', '##s', '.', '.', '.', ')', '−', '##Ho', '##w', 'can', '##wei', '##dentif', '##y', 'relevant', 'know', '##led', '##ge', 'bas', '##e', 'ent', '##ries', '?', 'quer', '##yan', '##sw', '##erw', '##rong', 'ans', '##wer', 'Eu', '##cl', '##id', '##ian', 'Dist', '##ance', 'Cos', '##ine', 'of', 'an', '##gl', '##e', 'between', 've', '##ctor', '##s', '18', '.', '03', '.', '202', '##4', 'NL', '##P', '–', 'We', '##ek', '4', 'Sem', '##anti', '##cs', '4', '##T', '##ime', 'flie', '##s', 'like', 'an', 'ar', '##row', ';', 'fr', '##ui', '##t', 'flie', '##s', 'like', 'a', 'ba', '##na', '##na', 'fl', '##y', ':', 'mov', '##ing', 'thr', '##ough', 'the', '##air', '.', 'fl', '##y', ':', 'a', 'sm', '##all', 'ins', '##ect', 'with', '##sm', '##all', 'win', '##gs', 'https', ':', '/', '/', 'web', '.', 'st', '##an', '##ford', '.', 'ed', '##u', '/', '~', '[SEP]']\n",
      "Chunk 2: ['[CLS]', 'nu', '##ts', '##hell', ':', 'A', 'fund', '##ament', '##al', 'as', '##pe', '##ct', 'of', 'sem', '##anti', '##cs', 'is', 'the', 're', '##co', '##gn', '##ition', 'that', 'there', 'is', 'not', 'al', '##ways', 'a', 'dir', '##ect', 'con', '##ne', '##ction', 'between', 'wor', '##ds', 'and', 'their', 'me', '##ani', '##ng', '##s', '.', 'This', 'con', '##cept', 'ch', '##allen', '##ges', 'the', 'int', '##ui', '##tive', 'belief', 'that', 'wor', '##ds', 'are', 'in', '##here', '##nt', '##ly', 'ti', '##ed', 'to', 'what', 'they', 're', '##pres', '##ent', '.', '18', '.', '03', '.', '202', '##4', 'NL', '##P', '–', 'We', '##ek', '4', 'Word', 'Rep', '##res', '##enta', '##tion', '5', 'Emp', '##ty', 'fiel', '##d', 'for', 'do', '##cu', '##ment', 'title', '12', '.', '1', '.', '2022', '##−', '##Ma', '##chin', '##e', 'Le', '##arning', 'Model', '##s', 'can', '##not', 're', '##ad', 'raw', 'te', '##xt', '−', '##L', '##Ms', 'see', '##m', 'to', '##bea', '##ble', '[SEP]']\n",
      "Chunk 3: ['[CLS]', 'based', 'on', 'char', '##act', '##ers', 'https', ':', '/', '/', 'www', '.', 'guten', '##berg', '.', 'or', '##g', '/', 'eb', '##ook', '##s', '/', '270', '##1', 'Tok', '##eni', '##za', '##tion', '6', '##−', '##Original', '##ly', 'the', '##ta', '##sk', 'of', '##seg', '##ment', '##ing', 'a', 'te', '##xt', '##int', '##owo', '##rd', '##s', '−', '##To', '##ken', '##ization', ':', 'The', 'pro', '##cess', 'of', '##seg', '##ment', '##ing', 'do', '##cu', '##ments', 'into', '##par', '##agra', '##ph', '##s', ',', 'sen', '##ten', '##ces', ',', 'and', 'wor', '##ds', '(', 'to', '##ken', '##s', ')', 'per', 'sen', '##ten', '##ce', '.', 'Text', '„', 'A', 'bo', '##ile', '##d', 'eg', '##g', 'in', 'them', '##orn', '##ing', 'is', '##hard', 'to', '##bea', '##t', '.', '“', 'Tok', '##ens', '„', 'A', '“', ',', '„', 'bo', '##ile', '##d', '“', ',', '„', 'eg', '##g', '“', ',', '„', 'in', '“', ',', '„', 'the', '“', '[SEP]']\n",
      "Chunk 4: ['[CLS]', 'to', '##ken', '##ization', ':', 'sim', '##ples', '##t', 'heu', '##rist', '##ic', 'to', '##spl', '##itte', '##xt', 'for', '##so', '##me', 'lang', '##uage', '##s', 'Source', ':', 'Carsten', '##sen', 'et', 'al', '.', ',', 'Computer', '##ling', '##ui', '##sti', '##k', 'und', 'Sprach', '##technologie', ':', 'Eine', 'Einführung', ',', 'Spektrum', 'Akadem', '##ischer', 'Verlag', 'Heidelberg', '[SEP]']\n",
      "Chunk 5: ['[CLS]', 'sim', '##ples', '##t', 'heu', '##rist', '##ic', 'to', '##spl', '##itte', '##xt', 'for', '##so', '##me', 'lang', '##uage', '##s', 'Source', ':', 'Carsten', '##sen', 'et', 'al', '.', ',', 'Computer', '##ling', '##ui', '##sti', '##k', 'und', 'Sprach', '##technologie', ':', 'Eine', 'Einführung', ',', 'Spektrum', 'Akadem', '##ischer', 'Verlag', 'Heidelberg', '18', '.', '03', '.', '202', '##4', 'NL', '##P', '–', 'We', '##ek', '4', 'Problem', ':', 'New', 'Word', '##s', '7', '##Problem', ':', 'You', '##bu', '##ilt', 'your', 'perfe', '##ct', 'lin', '##gu', '##ist', '##ic', 'to', '##ken', '##izer', 'on', 'a', 'train', '##ing', 'cor', '##pus', 'and', 'your', 'ac', '##tu', '##al', 'dat', '##a', 'that', '##yo', '##uw', '##ork', 'with', ',', 'con', '##tains', 'a', 'wor', '##d', 'it', '##has', '##not', 'see', '##n', 'bef', '##ore', '.', 'Ex', '##amp', '##le', 'Training', 'cor', '##pus', 'con', '##tains', ':', 'lo', '##w', ',', 'high', ',', 'ca', '##t', ',', 'st', '##air', '##s', '[SEP]']\n",
      "Chunk 6: ['[CLS]', 'world', 'app', '##li', '##cation', '##s', '?', '18', '.', '03', '.', '202', '##4', 'NL', '##P', '–', 'We', '##ek', '4', 'Sol', '##ution', ':', 'Sub', '##word', 'to', '##ken', '##isation', '8', '##Mode', '##rn', 'to', '##ken', '##izer', '##s', 'ind', '##uc', '##e', 'ideal', 'to', '##ken', '##s', 'that', '##are', '##oft', '##en', 'sm', '##aller', 'than', 'wor', '##ds', ',', 'cal', '##led', 'sub', '##word', '##s', '.', 'The', '##y', 'may', 'cor', '##respond', 'to', '##mor', '##phe', '##mes', '##1', ',', 'but', 'they', 'don', '‘', 't', 'have', 'to', '.', 'By', '##te', '-', 'Pa', '##ir', 'En', '##co', '##ding', ':', 'Bott', '##om', '-', 'up', '##app', '##ro', '##ach', ',', 'we', '##use', '##our', '##data', 'tot', '##ell', '##us', '##wh', '##at', 'the', '##ide', '##al', 'to', '##ken', 'sho', '##uld', 'be', '1', 'sm', '##alle', '##st', 'un', '##ito', '##fm', '##ean', '##ing', 'in', 'a', 'wor', '##dh', '##ttp', '##s', ':', '/', '[SEP]']\n",
      "Chunk 7: ['[CLS]', '–', 'We', '##ek', '4', 'By', '##te', '-', 'Pa', '##ir', 'En', '##co', '##ding', '9', '##−', '##To', '##ken', '-', 'le', '##arn', '##er', 'beg', '##ins', 'with', 'a', 'set', 'of', 'to', '##ken', '##s', 'that', 'is', 'just', 'a', 'set', 'of', 'all', 'individ', '##ual', 'char', '##act', '##ers', '−', '##The', '##n', 'it', 'start', '##s', 'to', 'mer', '##ge', 'the', 'two', 'most', 'fre', '##quen', '##t', 'co', '-', 'o', '##cc', '##urr', '##ing', 'char', '##act', '##ers', 'to', '##get', '##her', '.', '−', '##It', 'con', '##tinu', '##es', 'to', 'co', '##unt', 'and', 'mer', '##ge', 'to', '##get', '##her', 'se', '##quen', '##ces', ',', 'cre', '##ating', 'more', 'and', 'long', '##er', 'se', '##quen', '##ces', '−', '##The', 'al', '##ogo', '##rit', '##hm', 'us', '##ual', '##ly', 'respe', '##cts', 'wor', '##d', 'bo', '##und', '##arie', '##s', 'if', 'exist', '##ing', '−', '##O', '##ften', 'end', '##s', 'up', 'le', '##arning', 'ent', '##ire', '[SEP]']\n",
      "Chunk 8: ['[CLS]', 'mer', '##ging', 'ru', '##les', '.', 'https', ':', '/', '/', 'web', '.', 'st', '##an', '##ford', '.', 'ed', '##u', '/', '~', 'jur', '##af', '##sky', '/', 'sl', '##p', '##3', '/', '2', '.', 'p', '##df', '18', '.', '03', '.', '202', '##4', 'NL', '##P', '–', 'We', '##ek', '4', 'From', 'Tok', '##ens', 'to', '##N', '##umber', '##s', '10', '##A', 'tradition', '##al', 'way', 'to', 'en', '##code', 'te', '##xt', '##s', 'as', 'number', '##s', ':', 'one', '-', 'hot', '-', 'en', '##co', '##ding', 'Ex', '##amp', '##le', ':', 'for', 'web', 'sea', '##rc', '##h', 'you', 'want', 'to', 'ma', '##tc', '##h', '“', 'London', 'ca', '##b', '”', 'in', 'do', '##cu', '##ments', 'con', '##tain', '##ing', '“', 'London', 'ta', '##xi', '”', 'Result', ':', 'ca', '##b', '=', '[', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '[SEP]']\n",
      "Chunk 9: ['[CLS]', 'th', '##ogo', '##na', '##l', '−', '##The', '##re', 'is', 'no', 'natur', '##al', 'not', '##ion', 'of', 'sim', '##ila', '##rit', '##y', 'for', 'one', '-', 'hot', '-', 'en', '##code', '##d', 've', '##ctor', '##s', '−', '##The', '##y', 'be', '##come', 'imm', '##ens', '##ely', 'sp', '##ars', '##e', 'with', 'a', 'la', '##rg', '##e', 'vo', '##ca', '##bul', '##ary', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#gleich wie oben, aber mit neuem Sprachmodell, explizit für deutsch.\n",
    "\n",
    "model_name = \"Sahajtomar/German-semantic\"    #einzige Änderung zu oben: Neues Modell\n",
    "model = SentenceTransformer(model_name)\n",
    "tokenized_chunks = []\n",
    "for i, text in enumerate(token_split_texts[:10]):\n",
    "    # Tokenize each chunk\n",
    "    encoded_input = model.tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = model.tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0].tolist())\n",
    "    tokenized_chunks.append(tokens)\n",
    "    print(f\"Chunk {i}: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating Embeddings for Each Chunk\n",
    "\n",
    "Now we convert each text chunk into a numerical embedding that captures its semantic meaning. These embeddings will be used for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51285/495563739.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")     #Hier erstellst du eine Instanz von HuggingFaceEmbeddings aus langchain, basierend auf dem Modell\n"
     ]
    }
   ],
   "source": [
    "#In diesem Codeausschnitt werden Text-Chunks in Vektoren (Embeddings) umgewandelt – ein zentraler Schritt in NLP-Anwendungen wie Suche, Clustering oder Klassifikation.\n",
    "\n",
    "#Embedding ist ein Vektor (also eine Liste von Zahlen), der den inhaltlichen Bedeutungsraum eines Texts in einer numerischen Form abbildet, die ein Modell verstehen kann.\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")     #Hier erstellst du eine Instanz von HuggingFaceEmbeddings aus langchain, basierend auf dem Modell\n",
    "chunk_embeddings = model.encode(token_split_texts, convert_to_numpy=True)     #Hier nutzt du stattdessen das SentenceTransformer-Modell (aus vorheriger Zelle, z. B. model = SentenceTransformer(...)) direkt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "## 5. Building a FAISS Vector Store\n",
    "\n",
    "FAISS is a powerful library for efficient similarity search. Here, we build an index from our embeddings. Remember, FAISS only stores the numerical vectors so we must keep our original text mapping separately.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "d = chunk_embeddings.shape[1]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings in FAISS index: 19\n"
     ]
    }
   ],
   "source": [
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(chunk_embeddings)\n",
    "print(\"Number of embeddings in FAISS index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "index_2 = faiss.read_index(\"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"rb\") as f:\n",
    "    token_split_texts_2 = pickle.load(f)\n",
    "print(len(token_split_texts_2))\n",
    "print(len(token_split_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Projecting Embeddings with UMAP\n",
    "\n",
    "To visualize high-dimensional embeddings, we use UMAP to project them into 2D space. You can project both the entire dataset and individual query embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "#Konkret verwendest du UMAP, um deine hochmodimensionalen Vektoren (z. B. 384-dimensionale Embeddings) in 2D zu transformieren, damit du sie später grafisch darstellen kannst (z. B. mit matplotlib).\n",
    "\n",
    "#Du trainierst UMAP auf deinen chunk_embeddings.\n",
    "#chunk_embeddings: Deine Vektoren, z. B. mit Form (24, 384)\n",
    "#UMAP(...): UMAP ist ein nicht-lineares Reduktionsverfahren, das ähnlich wie t-SNE funktioniert – aber oft schneller und strukturtreuer ist.\n",
    "#fit(...): Berechnet die 2D-Repräsentation auf Basis aller Embeddings\n",
    "\n",
    "# Fit UMAP on the full dataset embeddings\n",
    "umap_transform = umap.UMAP(random_state=0, transform_seed=0).fit(chunk_embeddings)\n",
    "\n",
    "def project_embeddings(embeddings, umap_transform):    #Eine Funktion, die beliebige Embeddings (z. B. auch neue später) in denselben 2D-Raum transformieren kann – basierend auf dem oben gelernten umap_transform.\n",
    "    \"\"\"\n",
    "    Project a set of embeddings using a pre-fitted UMAP transform.\n",
    "    \"\"\"\n",
    "    umap_embeddings = np.empty((len(embeddings), 2))    #Platzhalter für alle 2D-Projektionen (eine Zeile pro Embedding, 2 Spalten → x/y).\n",
    "    for i, embedding in enumerate(tqdm.tqdm(embeddings, desc=\"Projecting Embeddings\")):    #Du projizierst jedes Embedding einzeln in den 2D-Raum und speicherst das Ergebnis.\n",
    "        umap_embeddings[i] = umap_transform.transform([embedding])\n",
    "    return umap_embeddings    #Gibt das neue Array mit 2D-Koordinaten zurück – perfekt für Plotting oder Clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projecting Embeddings:   0%|          | 0/19 [00:00<?, ?it/s]/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "Projecting Embeddings: 100%|██████████| 19/19 [00:00<00:00, 404.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected dataset embeddings shape: (19, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Project the entire dataset embeddings\n",
    "\n",
    "#Du projizierst alle deine Chunk-Embeddings in den zweidimensionalen Raum, um sie visuell analysieren oder darstellen zu können.\n",
    "\n",
    "projected_dataset_embeddings = project_embeddings(chunk_embeddings, umap_transform)\n",
    "print(\"Projected dataset embeddings shape:\", projected_dataset_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Querying the Vector Store and Projecting Results\n",
    "\n",
    "We now define a retrieval function that takes a text query, embeds it, and searches our FAISS index for similar documents. We then project these result embeddings with UMAP.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Du definierst eine Funktion namens retrieve(...), die für eine Suchanfrage (Query) die ähnlichsten Text-Chunks aus deinem Dokument findet – basierend auf Vektor-Ähnlichkeit im Embedding-Raum.\n",
    "\n",
    "def retrieve(query, k=5):    #Die Funktion nimmt zwei Eingaben: query: Ein Text-String, z. B. \"Was ist die Energiewende?\", k: Anzahl der ähnlichsten Treffer, die zurückgegeben werden sollen (Standard = 5)\n",
    "    \"\"\"\n",
    "    Retrieve the top k similar text chunks and their embeddings for a given query.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)    #Deine Textanfrage wird in ein Embedding-Vektor umgewandelt – genauso wie du es vorher mit den Chunks gemacht hast.\n",
    "    distances, indices = index.search(query_embedding, k)      #Hier passiert die eigentliche Suche: index ist ein Vektorsuchindex mit FAISS erstellt, search(...) gibt zurück\n",
    "    retrieved_texts = [chunks[i] for i in indices[0]]    #Du holst dir die originalen Textstücke (Chunks), die zu den k ähnlichsten Embeddings gehören.\n",
    "    retrieved_embeddings = np.array([chunk_embeddings[i] for i in indices[0]])    #Du holst dir auch die Embedding-Vektoren dieser Top-k-Treffer.\n",
    "    return retrieved_texts, retrieved_embeddings, distances[0]    #Rückgabe: die besten Text-Chunks, ihre Embeddings, die zugehörigen Abstände (für Ähnlichkeitsbewertung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mmachine learning algorithms\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results, result_embeddings, distances = \u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRetrieved document preview:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(results[\u001b[32m0\u001b[39m][:\u001b[32m300\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mretrieve\u001b[39m\u001b[34m(query, k)\u001b[39m\n\u001b[32m      7\u001b[39m query_embedding = model.encode([query], convert_to_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m)    \u001b[38;5;66;03m#Deine Textanfrage wird in ein Embedding-Vektor umgewandelt – genauso wie du es vorher mit den Chunks gemacht hast.\u001b[39;00m\n\u001b[32m      8\u001b[39m distances, indices = index.search(query_embedding, k)      \u001b[38;5;66;03m#Hier passiert die eigentliche Suche: index ist ein Vektorsuchindex mit FAISS erstellt, search(...) gibt zurück\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m retrieved_texts = [\u001b[43mchunks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices[\u001b[32m0\u001b[39m]]    \u001b[38;5;66;03m#Du holst dir die originalen Textstücke (Chunks), die zu den k ähnlichsten Embeddings gehören.\u001b[39;00m\n\u001b[32m     10\u001b[39m retrieved_embeddings = np.array([chunk_embeddings[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices[\u001b[32m0\u001b[39m]])    \u001b[38;5;66;03m#Du holst dir auch die Embedding-Vektoren dieser Top-k-Treffer.\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retrieved_texts, retrieved_embeddings, distances[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "query = \"machine learning algorithms\"\n",
    "results, result_embeddings, distances = retrieve(query, k=3)\n",
    "print(\"Retrieved document preview:\")\n",
    "print(results[0][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Project the result embeddings\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m projected_result_embeddings = project_embeddings(\u001b[43mresult_embeddings\u001b[49m, umap_transform)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Also embed and project the original query for visualization\u001b[39;00m\n\u001b[32m      5\u001b[39m query_embedding = model.encode([query], convert_to_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'result_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# Project the result embeddings\n",
    "projected_result_embeddings = project_embeddings(result_embeddings, umap_transform)\n",
    "\n",
    "# Also embed and project the original query for visualization\n",
    "query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "project_original_query = project_embeddings(query_embedding, umap_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizing the Results\n",
    "\n",
    "Finally, we create a scatter plot to visualize the entire dataset, the retrieved results, and the original query in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'projected_result_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Scatter plots\u001b[39;00m\n\u001b[32m      8\u001b[39m plt.scatter(projected_dataset_embeddings[:, \u001b[32m0\u001b[39m], projected_dataset_embeddings[:, \u001b[32m1\u001b[39m],\n\u001b[32m      9\u001b[39m             s=\u001b[32m10\u001b[39m, color=\u001b[33m'\u001b[39m\u001b[33mgray\u001b[39m\u001b[33m'\u001b[39m, label=\u001b[33m'\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m plt.scatter(\u001b[43mprojected_result_embeddings\u001b[49m[:, \u001b[32m0\u001b[39m], projected_result_embeddings[:, \u001b[32m1\u001b[39m],\n\u001b[32m     11\u001b[39m             s=\u001b[32m100\u001b[39m, facecolors=\u001b[33m'\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m'\u001b[39m, edgecolors=\u001b[33m'\u001b[39m\u001b[33mg\u001b[39m\u001b[33m'\u001b[39m, label=\u001b[33m'\u001b[39m\u001b[33mResults\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     12\u001b[39m plt.scatter(project_original_query[:, \u001b[32m0\u001b[39m], project_original_query[:, \u001b[32m1\u001b[39m],\n\u001b[32m     13\u001b[39m             s=\u001b[32m150\u001b[39m, marker=\u001b[33m'\u001b[39m\u001b[33mX\u001b[39m\u001b[33m'\u001b[39m, color=\u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, label=\u001b[33m'\u001b[39m\u001b[33mOriginal Query\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# If results is a list of texts, iterate directly\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'projected_result_embeddings' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def shorten_text(text, max_length=15):\n",
    "    \"\"\"Shortens text to max_length and adds an ellipsis if shortened.\"\"\"\n",
    "    return (text[:max_length] + '...') if len(text) > max_length else text\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Scatter plots\n",
    "plt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1],\n",
    "            s=10, color='gray', label='Dataset')\n",
    "plt.scatter(projected_result_embeddings[:, 0], projected_result_embeddings[:, 1],\n",
    "            s=100, facecolors='none', edgecolors='g', label='Results')\n",
    "plt.scatter(project_original_query[:, 0], project_original_query[:, 1],\n",
    "            s=150, marker='X', color='r', label='Original Query')\n",
    "\n",
    "# If results is a list of texts, iterate directly\n",
    "for i, text in enumerate(results):\n",
    "    if i < len(projected_result_embeddings):\n",
    "        plt.annotate(shorten_text(text),\n",
    "                     (projected_result_embeddings[i, 0], projected_result_embeddings[i, 1]),\n",
    "                     fontsize=8)\n",
    "\n",
    "# Annotate the original query point\n",
    "original_query_text = 'Original Query Text'  # Replace with your actual query text if needed\n",
    "plt.annotate(shorten_text(original_query_text),\n",
    "             (project_original_query[0, 0], project_original_query[0, 1]),\n",
    "             fontsize=8)\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('Hogwarts')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach Retrieved Results to LLM\n",
    "\n",
    "- Combine learnings from week 1 with approach from this week to inject your data into prompts and create a simple question answering system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
